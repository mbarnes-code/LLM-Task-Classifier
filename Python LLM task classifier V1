#!/usr/bin/env python
"""
Training Data Preparation Script for Airflow DAG Ingestion

This script will:
  • Print instructions on the expected data format.
  • Prompt the user to specify a folder that contains documents (PDF, TXT, DOCX).
  • Scan and extract text from each document and automatically parse out subjects.
  • For each subject, generate “tasks” (each with fields: task_description, domain, recommended_execution).
    – Tasks are generated until a minimum of 50,000 examples (up to 100,000) per subject.
    – Tasks are generated in batches for performance.
  • Monitor for bias in the training data. If any domain (task “class”) exceeds its expected share by >10%,
    a rebalancing procedure is automatically triggered.
    – If rebalancing happens more than 5 times before reaching 5000 tasks, the script will stop and ask for more data.
  • Periodically check if the dataset for any subject exceeds 50GB on disk and print a message once it’s ready for LLM fine-tuning.
  • Use the distilbert-base-uncased model for fast inference.
  • Support FP16 operations on CUDA-enabled GPUs (users may disable FP16 with a command-line flag) and automatically fall back to CPU.
  • Output separate Parquet files (one per subject) ready to be ingested into an Airflow DAG.

Usage:
    python prepare_training_data.py [--no_fp16]

Make sure you have installed the required packages:
    pip install pandas pyarrow torch transformers PyPDF2 python-docx
"""

import os
import glob
import argparse
import random
import sys

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel

# Optional imports for file reading:
try:
    import PyPDF2
except ImportError:
    print("Please install PyPDF2 for PDF support (pip install PyPDF2)")
    sys.exit(1)

try:
    import docx
except ImportError:
    print("Please install python-docx for Word document support (pip install python-docx)")
    sys.exit(1)


def print_instructions():
    instructions = """
    Welcome to the Training Data Preparation Script for Airflow DAG ingestion.

    Expected data:
      • A folder containing documents in PDF, TXT, or DOCX format.
      • Each document should include subject markers (e.g., lines starting with "Subject:") 
        to indicate the subject matter.
      • The script will parse each document, extract subjects, and generate tasks with the format:
            "task_description", "domain", "recommended_execution"
      • Tasks will be generated in batches until between 50,000 and 100,000 examples per subject are reached.
      • The script includes automatic bias detection and rebalancing (if any domain exceeds its expected share by >10%).
      • Dataset size is monitored, and once a subject’s dataset exceeds 50GB, a message will indicate it is ready for LLM fine-tuning.
      
    Additional Options:
      • The script uses the distilbert-base-uncased model for fast inference.
      • FP16 (half precision) operations are enabled by default on CUDA-enabled GPUs.
        Use the '--no_fp16' flag if your GPU does not support FP16.
      
    Please ensure your data meets these requirements before continuing.
    """
    print(instructions)


def get_folder():
    folder = input("Please enter the path to the folder containing your documents: ").strip()
    return folder


def extract_text(file_path):
    """Extract text from a file based on its extension."""
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".txt":
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return ""
    elif ext == ".pdf":
        text = ""
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
        except Exception as e:
            print(f"Error reading PDF {file_path}: {e}")
        return text
    elif ext == ".docx":
        try:
            doc = docx.Document(file_path)
            return "\n".join([p.text for p in doc.paragraphs])
        except Exception as e:
            print(f"Error reading DOCX {file_path}: {e}")
            return ""
    else:
        print(f"Unsupported file type: {file_path}")
        return ""


def parse_subjects(text):
    """
    Parse subjects from document text.
    This demo looks for lines starting with 'Subject:'.
    If none found, all text is assigned to a default 'General' subject.
    """
    subjects = {}
    current_subject = "General"
    lines = text.splitlines()
    for line in lines:
        if line.startswith("Subject:"):
            current_subject = line.replace("Subject:", "").strip()
            if current_subject not in subjects:
                subjects[current_subject] = ""
        else:
            subjects.setdefault(current_subject, "")
            subjects[current_subject] += line + "\n"
    return subjects


def generate_tasks_for_subject(subject, content, model, tokenizer, device, batch_size=100):
    """
    Simulate generating tasks for a subject.
    Each task is a dict with keys: task_description, domain, recommended_execution.
    
    For demonstration purposes, this function uses a simple randomized approach.
    In practice, you would use your model to extract meaningful tasks from the content.
    """
    tasks = []
    rebalancing_count = 0
    domain_distribution = {}  # Counts per domain
    # For simulation, assume these candidate domains (could be derived from content in a real use-case)
    candidate_domains = ["Finance", "HR", "IT", "Operations", "Sales"]

    # Split the content into paragraphs as a simple way to get text chunks.
    paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]
    if not paragraphs:
        paragraphs = [content]

    total_tasks = 0
    target_min = 50000
    target_max = 100000

    while total_tasks < target_min:
        batch_tasks = []
        # Process a batch of tasks
        for _ in range(batch_size):
            # Randomly select a paragraph (simulate batch processing)
            paragraph = random.choice(paragraphs)
            # In a real scenario, you might run the model over the paragraph to generate a task description.
            task_description = f"Review content snippet: {paragraph[:50]}..."
            # For demo, randomly assign a domain from our candidate list.
            domain = random.choice(candidate_domains)
            recommended_execution = "automated analysis" if len(paragraph) > 100 else "manual review"
            task = {
                "task_description": task_description,
                "domain": domain,
                "recommended_execution": recommended_execution,
            }
            batch_tasks.append(task)
            domain_distribution[domain] = domain_distribution.get(domain, 0) + 1
        tasks.extend(batch_tasks)
        total_tasks = len(tasks)

        # --- Bias Detection and Correction ---
        # Here we check if any domain’s count is too high relative to the expected equal share.
        expected_share = total_tasks / len(candidate_domains)
        bias_detected = False
        for d, count in domain_distribution.items():
            # If a domain exceeds expected share by >10% of total tasks, consider it biased.
            if count > expected_share + 0.1 * total_tasks:
                bias_detected = True
                # Rebalance: downsample tasks from this domain to the allowed limit.
                allowed = int(expected_share + 0.1 * total_tasks)
                # Filter tasks to remove extra occurrences of the dominant domain.
                new_tasks = []
                current_count = 0
                for t in tasks:
                    if t["domain"] == d:
                        if current_count < allowed:
                            new_tasks.append(t)
                            current_count += 1
                    else:
                        new_tasks.append(t)
                tasks = new_tasks
                domain_distribution[d] = allowed
                rebalancing_count += 1
                print(f"Bias detected in domain '{d}'. Rebalancing... (Rebalance count: {rebalancing_count})")
                break  # re-check in the next batch

        if rebalancing_count > 5 and total_tasks < 5000:
            print("Rebalancing occurred more than 5 times before 5000 tasks were generated. Please provide more data.")
            sys.exit(1)

        # (Optional: break early if we exceed the target_max)
        if total_tasks >= target_max:
            tasks = tasks[:target_max]
            break

    return tasks


def check_dataset_size(subject, tasks, output_folder):
    """
    Write current tasks to a temporary Parquet file and check its file size.
    If the file size exceeds 50GB, return the size in GB.
    """
    df = pd.DataFrame(tasks)
    temp_path = os.path.join(output_folder, f"{subject}_temp.parquet")
    df.to_parquet(temp_path, index=False)
    size_bytes = os.path.getsize(temp_path)
    size_gb = size_bytes / (1024**3)
    os.remove(temp_path)
    return size_gb


def main():
    print_instructions()
    folder = get_folder()
    if not os.path.isdir(folder):
        print("The provided folder path is not valid.")
        sys.exit(1)

    # Create an output folder for the Parquet files.
    output_folder = os.path.join(folder, "parquet_output")
    os.makedirs(output_folder, exist_ok=True)

    # Find all supported document files.
    file_extensions = ("*.txt", "*.pdf", "*.docx")
    file_paths = []
    for ext in file_extensions:
        file_paths.extend(glob.glob(os.path.join(folder, ext)))
    if not file_paths:
        print("No supported document files found in the folder.")
        sys.exit(1)

    # Extract text from each file and parse subjects.
    subjects_dict = {}
    for file in file_paths:
        print(f"Processing file: {file}")
        text = extract_text(file)
        subj_texts = parse_subjects(text)
        for subj, content in subj_texts.items():
            if subj in subjects_dict:
                subjects_dict[subj] += "\n" + content
            else:
                subjects_dict[subj] = content

    # Process command-line arguments for FP16 usage.
    parser = argparse.ArgumentParser(description="Training Data Preparation Script")
    parser.add_argument("--no_fp16", action="store_true", help="Disable FP16 training")
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    use_fp16 = (not args.no_fp16) and (device.type == "cuda")
    print(f"Using device: {device}. FP16 enabled: {use_fp16}")

    # Load the distilbert-base-uncased model and tokenizer.
    model_name = "distilbert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if use_fp16:
        model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16).to(device)
    else:
        model = AutoModel.from_pretrained(model_name).to(device)

    # For each subject, generate tasks and create a Parquet file.
    for subject, content in subjects_dict.items():
        print(f"\nGenerating tasks for subject: {subject}")
        tasks = generate_tasks_for_subject(subject, content, model, tokenizer, device)

        # Periodically check if the dataset for this subject exceeds 50GB.
        size_gb = check_dataset_size(subject, tasks, output_folder)
        if size_gb > 50:
            print(f"Dataset for subject '{subject}' exceeds 50GB. It is now ready for LLM fine-tuning.")
        else:
            print(f"Current dataset size for subject '{subject}': {size_gb:.2f} GB.")

        # Save the tasks to a Parquet file.
        df = pd.DataFrame(tasks)
        output_path = os.path.join(output_folder, f"{subject}.parquet")
        df.to_parquet(output_path, index=False)
        print(f"Parquet file saved for subject '{subject}' at: {output_path}")


if __name__ == "__main__":
    main()
