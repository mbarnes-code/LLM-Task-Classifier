#!/usr/bin/env python
"""
Production Training Data Preparation Script for Airflow DAG Ingestion

This script loads its configuration from a YAML file (e.g., "production_config.yaml")
and performs the following:
  - Processes documents from the input folder using concurrent file processing with a status bar.
  - Cleans and chunks text intelligently.
  - Summarizes each chunk using a transformer summarization model.
  - Automatically generates tasks based on the summary.
  - Once a fileâ€™s worth of tasks (at least min_tasks, up to max_tasks) is generated,
    it writes the tasks to a new Parquet file (e.g., subject_1.parquet, subject_2.parquet, etc.).
  - Continues generating tasks for each subject until all text chunks have been processed.
  - Logs progress and errors as configured.

Usage:
    python prepare_training_data.py --config production_config.yaml [--no_fp16]

Ensure required packages are installed:
    pip install pandas pyarrow torch transformers PyPDF2 python-docx pyyaml tqdm
"""

import os
import glob
import argparse
import random
import sys
import logging
import yaml
from logging.handlers import RotatingFileHandler
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel, pipeline

# Optional imports for file reading:
try:
    import PyPDF2
except ImportError:
    print("Please install PyPDF2 for PDF support (pip install PyPDF2)")
    sys.exit(1)

try:
    import docx
except ImportError:
    print("Please install python-docx for DOCX support (pip install python-docx)")
    sys.exit(1)

try:
    from tqdm import tqdm
except ImportError:
    tqdm = lambda x, **kwargs: x  # Fallback if tqdm is not installed

#########################################
# Configuration, Logging & Utility Functions
#########################################

def load_config(config_path):
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

def setup_logging(logging_config):
    log_level = getattr(logging, logging_config.get("level", "INFO").upper(), logging.INFO)
    logger = logging.getLogger()
    logger.setLevel(log_level)
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

    # File handler with rotation if enabled
    if logging_config.get("log_file"):
        log_file = logging_config["log_file"]
        if logging_config.get("rotate", False):
            max_bytes = parse_size(logging_config.get("max_file_size", "100MB"))
            backup_count = logging_config.get("backup_count", 5)
            file_handler = RotatingFileHandler(log_file, maxBytes=max_bytes, backupCount=backup_count)
        else:
            file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    if logging_config.get("enable_console", True):
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

def parse_size(size_str):
    size_str = size_str.strip().upper()
    if size_str.endswith("GB"):
        return int(float(size_str[:-2]) * 1024**3)
    elif size_str.endswith("MB"):
        return int(float(size_str[:-2]) * 1024**2)
    elif size_str.endswith("KB"):
        return int(float(size_str[:-2]) * 1024)
    else:
        return int(size_str)

def print_instructions():
    instructions = """
    Production Training Data Preparation Script for Airflow DAG Ingestion.
    The script processes documents from the input folder, extracts text,
    cleans and chunks it, summarizes each chunk, and generates tasks.
    Each file of tasks (between min_tasks and max_tasks tasks) is written as a new Parquet file.
    Progress is shown via status bars.
    """
    print(instructions)
    logging.info("Instructions printed.")

def get_input_folder(config):
    input_folder = config.get("storage", {}).get("input_folder", "")
    if not input_folder:
        input_folder = input("Please enter the path to the folder containing your documents: ").strip()
    return input_folder

#########################################
# File Processing and Text Extraction
#########################################

def extract_text(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".txt":
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logging.error(f"Error reading {file_path}: {e}")
            return ""
    elif ext == ".pdf":
        text = ""
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
        except Exception as e:
            logging.error(f"Error reading PDF {file_path}: {e}")
        return text
    elif ext == ".docx":
        try:
            doc = docx.Document(file_path)
            return "\n".join([p.text for p in doc.paragraphs])
        except Exception as e:
            logging.error(f"Error reading DOCX {file_path}: {e}")
            return ""
    else:
        logging.warning(f"Unsupported file type: {file_path}")
        return ""

def clean_and_chunk_text(text, max_chunk_chars=500):
    """
    Clean the text and split it into smaller chunks.
    First, split by double newlines (paragraphs). Then, for any long paragraph,
    further split by period+space.
    """
    chunks = []
    paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
    for para in paragraphs:
        if len(para) > max_chunk_chars:
            sentences = para.split(". ")
            current_chunk = ""
            for sentence in sentences:
                if len(current_chunk) + len(sentence) + 1 <= max_chunk_chars:
                    current_chunk += sentence + ". "
                else:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentence + ". "
            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            chunks.append(para)
    return chunks

#########################################
# Summarization and Task Generation
#########################################

def initialize_summarizer(config):
    summarizer_model = config.get("model_config", {}).get("summarizer_model", "facebook/bart-large-cnn")
    summarizer = pipeline("summarization", model=summarizer_model)
    return summarizer

def summarize_chunk(text_chunk, summarizer, max_length=130, min_length=30):
    try:
        # Truncate long text chunks to avoid model errors.
        if len(text_chunk.split()) > 512:
            text_chunk = " ".join(text_chunk.split()[:512])
        summary = summarizer(text_chunk, max_length=max_length, min_length=min_length, do_sample=False)
        return summary[0]['summary_text']
    except Exception as e:
        logging.error(f"Summarization failed: {e}")
        return text_chunk[:100] + "..."

def filter_duplicate_tasks(tasks):
    seen = set()
    unique_tasks = []
    for task in tasks:
        desc = task["task_description"]
        if desc not in seen:
            seen.add(desc)
            unique_tasks.append(task)
    return unique_tasks

def generate_and_store_tasks_for_subject(subject, content, config, summarizer, output_folder):
    """
    Generate tasks from a subject's content by processing its text chunks.
    Instead of stopping when a minimum threshold is reached, this function writes out a new Parquet file
    each time the accumulated tasks reach between min_tasks and max_tasks.
    The process continues until all chunks have been processed.
    """
    min_tasks = config["task_generation"].get("min_tasks_per_subject", 50000)
    max_tasks = config["task_generation"].get("max_tasks_per_subject", 100000)
    max_iterations = config["task_generation"].get("max_iterations", 1)
    candidate_domains = list(config.get("subject_extraction", {}).get("keywords", {}).keys())
    
    # Clean and chunk the content.
    chunks = clean_and_chunk_text(content)
    logging.info(f"Subject '{subject}' has {len(chunks)} text chunks.")
    
    file_count = 1
    # For status reporting: total chunks processed.
    processed_chunks = 0
    
    for iteration in range(max_iterations):
        current_tasks = []
        # Use a progress bar for chunk processing.
        for chunk in tqdm(chunks, desc=f"[{subject}] Iteration {iteration+1} processing chunks", leave=False):
            processed_chunks += 1
            summary = summarize_chunk(chunk, summarizer)
            task_description = f"Analyze the following summary: '{summary}'"
            domain = random.choice(candidate_domains)
            recommended_execution = "automated analysis" if len(chunk) > 100 else "manual review"
            current_tasks.append({
                "task_description": task_description,
                "domain": domain,
                "recommended_execution": recommended_execution,
            })
            # If we've reached the max_tasks threshold, flush current_tasks to a new file.
            if len(current_tasks) >= max_tasks:
                current_tasks = filter_duplicate_tasks(current_tasks)
                filename = os.path.join(output_folder, f"{subject}_{file_count}.parquet")
                pd.DataFrame(current_tasks).to_parquet(filename, index=False)
                logging.info(f"Saved {len(current_tasks)} tasks to {filename}")
                file_count += 1
                current_tasks = []
        # After processing all chunks in this iteration, if current_tasks are non-empty and above a minimal threshold:
        if current_tasks:
            if len(current_tasks) < min_tasks:
                logging.info(f"Iteration {iteration+1} for subject '{subject}' produced only {len(current_tasks)} tasks; flushing remaining tasks.")
            current_tasks = filter_duplicate_tasks(current_tasks)
            filename = os.path.join(output_folder, f"{subject}_{file_count}.parquet")
            pd.DataFrame(current_tasks).to_parquet(filename, index=False)
            logging.info(f"Saved {len(current_tasks)} tasks to {filename}")
            file_count += 1
    logging.info(f"Finished generating tasks for subject '{subject}'. Total chunks processed: {processed_chunks}")

#########################################
# Parallel File Processing
#########################################

def process_file(file, subject_keywords):
    logging.info(f"Processing file: {file}")
    text = extract_text(file)
    if not text.strip():
        return None
    # Simple keyword matching to determine subject.
    text_lower = text.lower()
    counts = {}
    for subject, keywords in subject_keywords.items():
        count = sum(text_lower.count(keyword.lower()) for keyword in keywords)
        counts[subject] = count
    chosen_subject = max(counts, key=counts.get)
    if counts[chosen_subject] == 0:
        chosen_subject = "General"
    return (chosen_subject, text)

#########################################
# Main Function
#########################################

def main():
    parser = argparse.ArgumentParser(description="Production Training Data Preparation Script")
    parser.add_argument("--config", type=str, default="production_config.yaml", help="Path to YAML config file")
    parser.add_argument("--no_fp16", action="store_true", help="Disable FP16 training")
    args = parser.parse_args()

    # Load configuration and setup logging.
    config = load_config(args.config)
    setup_logging(config.get("logging", {}))
    logging.info("Configuration loaded and logging set up.")

    print_instructions()

    # Get input folder from config or prompt user.
    input_folder = get_input_folder(config)
    if not os.path.isdir(input_folder):
        logging.error("The provided folder path is not valid.")
        sys.exit(1)

    # Create output folder from configuration.
    output_folder = config.get("storage", {}).get("output_folder", os.path.join(input_folder, "parquet_output"))
    os.makedirs(output_folder, exist_ok=True)

    # Find supported document files.
    file_extensions = ("*.txt", "*.pdf", "*.docx")
    file_paths = []
    for ext in file_extensions:
        file_paths.extend(glob.glob(os.path.join(input_folder, ext)))
    if not file_paths:
        logging.error("No supported document files found in the folder.")
        sys.exit(1)

    # Process files in parallel to extract text and determine subject.
    subject_keywords = config.get("subject_extraction", {}).get("keywords", {})
    subjects_dict = {}
    with ThreadPoolExecutor(max_workers=config.get("parallel_processing", {}).get("num_workers", 4)) as executor:
        future_to_file = {executor.submit(process_file, file, subject_keywords): file for file in file_paths}
        for future in tqdm(as_completed(future_to_file), total=len(file_paths), desc="Ingesting files"):
            result = future.result()
            if result is None:
                continue
            subject, text = result
            subjects_dict.setdefault(subject, "")
            subjects_dict[subject] += "\n" + text

    # Configure device and FP16 usage.
    device_config = config.get("device_config", {})
    if device_config.get("device", "auto") == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(device_config.get("device"))
    use_fp16 = (device.type == "cuda" and device_config.get("enable_fp16", True) and not args.no_fp16)
    logging.info(f"Using device: {device}. FP16 enabled: {use_fp16}")

    # Load model and tokenizer.
    model_name = config.get("model_config", {}).get("model_name", "distilbert-base-uncased")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if use_fp16:
        model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16).to(device)
    else:
        model = AutoModel.from_pretrained(model_name).to(device)

    # Initialize summarizer.
    summarizer = initialize_summarizer(config)

    # For each subject, generate and store tasks in separate Parquet files.
    for subject, content in subjects_dict.items():
        logging.info(f"Generating tasks for subject: {subject}")
        generate_and_store_tasks_for_subject(subject, content, config, summarizer, output_folder)

if __name__ == "__main__":
    main()
