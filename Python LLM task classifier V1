#!/usr/bin/env python
"""
Training Data Preparation Script for Airflow DAG Ingestion

This script will:
  • Print instructions on the expected data format.
  • Prompt the user to specify a folder containing documents (PDF, TXT, DOCX).
  • Scan each document, extract its text, and determine its subject based on keyword matching.
  • For each subject, generate “tasks” with fields: task_description, domain, recommended_execution.
    – Tasks are generated in batches until between 50,000 and 100,000 examples per subject are reached.
    – New tasks are appended to a permanent Parquet file for that subject.
  • Monitor and automatically rebalance training data if any domain exceeds 10% dominance.
    – If rebalancing occurs more than 5 times before reaching 5000 tasks, the script stops with a message.
  • Periodically check if a subject’s dataset exceeds 50GB and print a message when it’s ready for LLM fine-tuning.
  • Use the distilbert-base-uncased model for fast inference.
  • Support FP16 operations on CUDA-enabled GPUs (users may disable FP16 with a flag) with smooth CPU fallback.
  
Usage:
    python prepare_training_data.py [--no_fp16]

Ensure you have installed the required packages:
    pip install pandas pyarrow torch transformers PyPDF2 python-docx
"""

import os
import glob
import argparse
import random
import sys

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel

# Optional imports for file reading:
try:
    import PyPDF2
except ImportError:
    print("Please install PyPDF2 for PDF support (pip install PyPDF2)")
    sys.exit(1)

try:
    import docx
except ImportError:
    print("Please install python-docx for Word document support (pip install python-docx)")
    sys.exit(1)


def print_instructions():
    instructions = """
    Welcome to the Training Data Preparation Script for Airflow DAG ingestion.

    Expected data:
      • A folder containing documents in PDF, TXT, or DOCX format.
      • Each document will be read in full and the subject will be determined based on keyword matching.
      • The script will generate tasks with the format:
            "task_description", "domain", "recommended_execution"
      • Tasks will be generated in batches until between 50,000 and 100,000 examples per subject are reached.
      • New tasks are appended to a permanent Parquet file for each subject.
      • The script includes automatic bias detection and rebalancing (if any domain exceeds its expected share by >10%).
      • Dataset size is monitored, and once a subject’s dataset exceeds 50GB, a message indicates it is ready for LLM fine-tuning.
      
    Additional Options:
      • The script uses the distilbert-base-uncased model for fast inference.
      • FP16 (half precision) operations are enabled by default on CUDA-enabled GPUs.
        Use the '--no_fp16' flag if your GPU does not support FP16.
      
    Please ensure your data meets these requirements before continuing.
    """
    print(instructions)


def get_folder():
    folder = input("Please enter the path to the folder containing your documents: ").strip()
    return folder


def extract_text(file_path):
    """Extract text from a file based on its extension."""
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".txt":
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return ""
    elif ext == ".pdf":
        text = ""
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
        except Exception as e:
            print(f"Error reading PDF {file_path}: {e}")
        return text
    elif ext == ".docx":
        try:
            doc = docx.Document(file_path)
            return "\n".join([p.text for p in doc.paragraphs])
        except Exception as e:
            print(f"Error reading DOCX {file_path}: {e}")
            return ""
    else:
        print(f"Unsupported file type: {file_path}")
        return ""


def determine_subject(text):
    """
    Determine the subject of a document based on keyword counts.
    Predefined subjects and their associated keywords:
      - Finance: bank, finance, investment, market
      - HR: human resource, employee, recruitment
      - IT: software, IT, technology, computer
      - Operations: operation, supply chain, logistics
      - Sales: sales, revenue, customer, market

    Returns the subject with the highest keyword count; if none match, returns 'General'.
    """
    subjects_keywords = {
         "Finance": ["bank", "finance", "investment", "market"],
         "HR": ["human resource", "employee", "recruitment"],
         "IT": ["software", "it", "technology", "computer"],
         "Operations": ["operation", "supply chain", "logistics"],
         "Sales": ["sales", "revenue", "customer", "market"]
    }
    counts = {}
    text_lower = text.lower()
    for subject, keywords in subjects_keywords.items():
         # Sum the occurrence of each keyword
         count = sum(text_lower.count(keyword) for keyword in keywords)
         counts[subject] = count
    subject_chosen = max(counts, key=counts.get)
    if counts[subject_chosen] == 0:
         return "General"
    return subject_chosen


def generate_tasks_for_subject(subject, content, model, tokenizer, device, batch_size=100):
    """
    Simulate generating tasks for a subject.
    Each task is a dict with keys: task_description, domain, recommended_execution.
    
    For demonstration purposes, this function uses a simple randomized approach.
    In practice, you would use your model to extract meaningful tasks from the content.
    """
    tasks = []
    rebalancing_count = 0
    domain_distribution = {}  # Counts per domain
    candidate_domains = ["Finance", "HR", "IT", "Operations", "Sales"]

    # Split the content into paragraphs as a simple way to get text chunks.
    paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]
    if not paragraphs:
        paragraphs = [content]

    total_tasks = 0
    target_min = 50000
    target_max = 100000

    while total_tasks < target_min:
        batch_tasks = []
        # Process a batch of tasks
        for _ in range(batch_size):
            paragraph = random.choice(paragraphs)
            task_description = f"Review content snippet: {paragraph[:50]}..."
            domain = random.choice(candidate_domains)
            recommended_execution = "automated analysis" if len(paragraph) > 100 else "manual review"
            task = {
                "task_description": task_description,
                "domain": domain,
                "recommended_execution": recommended_execution,
            }
            batch_tasks.append(task)
            domain_distribution[domain] = domain_distribution.get(domain, 0) + 1
        tasks.extend(batch_tasks)
        total_tasks = len(tasks)

        # --- Bias Detection and Correction ---
        expected_share = total_tasks / len(candidate_domains)
        for d, count in domain_distribution.items():
            if count > expected_share + 0.1 * total_tasks:
                # Rebalance: downsample tasks from the dominant domain.
                allowed = int(expected_share + 0.1 * total_tasks)
                new_tasks = []
                current_count = 0
                for t in tasks:
                    if t["domain"] == d:
                        if current_count < allowed:
                            new_tasks.append(t)
                            current_count += 1
                    else:
                        new_tasks.append(t)
                tasks = new_tasks
                domain_distribution[d] = allowed
                rebalancing_count += 1
                print(f"Bias detected in domain '{d}'. Rebalancing... (Rebalance count: {rebalancing_count})")
                break  # Re-check in the next batch

        if rebalancing_count > 5 and total_tasks < 5000:
            print("Rebalancing occurred more than 5 times before 5000 tasks were generated. Please provide more data.")
            sys.exit(1)

        # If we exceed the target_max, trim the tasks.
        if total_tasks >= target_max:
            tasks = tasks[:target_max]
            break

    return tasks


def check_dataset_size(file_path):
    """
    Check the file size of the given Parquet file in GB.
    """
    if not os.path.exists(file_path):
        return 0
    size_bytes = os.path.getsize(file_path)
    size_gb = size_bytes / (1024**3)
    return size_gb


def append_tasks_to_parquet(subject, new_tasks, output_folder):
    """
    Append new tasks to a permanent Parquet file for a given subject.
    If the file already exists, load it, append the new tasks, and overwrite the file.
    """
    df_new = pd.DataFrame(new_tasks)
    output_path = os.path.join(output_folder, f"{subject}.parquet")
    if os.path.exists(output_path):
        try:
            df_existing = pd.read_parquet(output_path)
            df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        except Exception as e:
            print(f"Error reading existing Parquet file for subject '{subject}': {e}")
            df_combined = df_new
    else:
        df_combined = df_new

    df_combined.to_parquet(output_path, index=False)
    return output_path, df_combined


def main():
    print_instructions()
    folder = get_folder()
    if not os.path.isdir(folder):
        print("The provided folder path is not valid.")
        sys.exit(1)

    # Create an output folder for the Parquet files.
    output_folder = os.path.join(folder, "parquet_output")
    os.makedirs(output_folder, exist_ok=True)

    # Find all supported document files.
    file_extensions = ("*.txt", "*.pdf", "*.docx")
    file_paths = []
    for ext in file_extensions:
        file_paths.extend(glob.glob(os.path.join(folder, ext)))
    if not file_paths:
        print("No supported document files found in the folder.")
        sys.exit(1)

    # Process each file: extract text and determine the subject based on keywords.
    subjects_dict = {}
    for file in file_paths:
        print(f"Processing file: {file}")
        text = extract_text(file)
        if not text.strip():
            continue
        subject = determine_subject(text)
        if subject in subjects_dict:
            subjects_dict[subject] += "\n" + text
        else:
            subjects_dict[subject] = text

    # Process command-line arguments for FP16 usage.
    parser = argparse.ArgumentParser(description="Training Data Preparation Script")
    parser.add_argument("--no_fp16", action="store_true", help="Disable FP16 training")
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    use_fp16 = (not args.no_fp16) and (device.type == "cuda")
    print(f"Using device: {device}. FP16 enabled: {use_fp16}")

    # Load the distilbert-base-uncased model and tokenizer.
    model_name = "distilbert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if use_fp16:
        model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16).to(device)
    else:
        model = AutoModel.from_pretrained(model_name).to(device)

    # For each subject, generate tasks, append them to the permanent Parquet file,
    # and check the file size.
    for subject, content in subjects_dict.items():
        print(f"\nGenerating tasks for subject: {subject}")
        new_tasks = generate_tasks_for_subject(subject, content, model, tokenizer, device)

        output_path, df_combined = append_tasks_to_parquet(subject, new_tasks, output_folder)
        size_gb = check_dataset_size(output_path)
        if size_gb > 50:
            print(f"Dataset for subject '{subject}' exceeds 50GB. It is now ready for LLM fine-tuning.")
        else:
            print(f"Current dataset size for subject '{subject}': {size_gb:.2f} GB.")

        print(f"Permanent Parquet file for subject '{subject}' saved at: {output_path}")


if __name__ == "__main__":
    main()
