#!/usr/bin/env python
"""
Production Training Data Preparation Script for Airflow DAG Ingestion

This script loads its configuration from a YAML file (e.g., "production_config.yaml")
and performs the following:
  - Processes documents from the input folder using concurrent file processing with status bars.
  - Cleans and chunks text intelligently.
  - Summarizes each chunk using a transformer summarization model.
  - Automatically generates tasks based on the summary.
  - Once the accumulated tasks for a subject reach the maximum threshold,
    writes the tasks to a new Parquet file (e.g., subject_1.parquet, subject_2.parquet, etc.).
  - After processing all chunks for a subject, flushes any remaining tasks.
  - Logs progress and errors as configured.

Usage:
    python prepare_training_data.py --config production_config.yaml [--no_fp16]

Ensure required packages are installed:
    pip install pandas pyarrow torch transformers PyPDF2 python-docx pyyaml tqdm
"""

import os
import glob
import argparse
import random
import sys
import logging
import yaml
from logging.handlers import RotatingFileHandler
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel, pipeline

# Optional imports for file reading:
try:
    import PyPDF2
except ImportError:
    print("Please install PyPDF2 for PDF support (pip install PyPDF2)")
    sys.exit(1)

try:
    import docx
except ImportError:
    print("Please install python-docx for DOCX support (pip install python-docx)")
    sys.exit(1)

try:
    from tqdm import tqdm
except ImportError:
    tqdm = lambda x, **kwargs: x  # Fallback if tqdm is not installed

#########################################
# Configuration, Logging & Utility Functions
#########################################

def load_config(config_path):
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

def setup_logging(logging_config):
    log_level = getattr(logging, logging_config.get("level", "INFO").upper(), logging.INFO)
    logger = logging.getLogger()
    logger.setLevel(log_level)
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

    # File handler with rotation if enabled
    if logging_config.get("log_file"):
        log_file = logging_config["log_file"]
        if logging_config.get("rotate", False):
            max_bytes = parse_size(logging_config.get("max_file_size", "100MB"))
            backup_count = logging_config.get("backup_count", 5)
            file_handler = RotatingFileHandler(log_file, maxBytes=max_bytes, backupCount=backup_count)
        else:
            file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    if logging_config.get("enable_console", True):
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

def parse_size(size_str):
    size_str = size_str.strip().upper()
    if size_str.endswith("GB"):
        return int(float(size_str[:-2]) * 1024**3)
    elif size_str.endswith("MB"):
        return int(float(size_str[:-2]) * 1024**2)
    elif size_str.endswith("KB"):
        return int(float(size_str[:-2]) * 1024)
    else:
        return int(size_str)

def print_instructions():
    instructions = """
    Production Training Data Preparation Script for Airflow DAG Ingestion.
    The script processes documents from the input folder, extracts text,
    cleans and chunks it, summarizes each chunk, and generates tasks.
    Each subject's tasks are incrementally saved to new Parquet files when the
    accumulated tasks reach the maximum threshold.
    Status bars show progress throughout the process.
    """
    print(instructions)
    logging.info("Instructions printed.")

def get_input_folder(config):
    input_folder = config.get("storage", {}).get("input_folder", "")
    if not input_folder:
        input_folder = input("Please enter the path to the folder containing your documents: ").strip()
    return input_folder

#########################################
# File Processing and Text Extraction
#########################################

def extract_text(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".txt":
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logging.error(f"Error reading {file_path}: {e}")
            return ""
    elif ext == ".pdf":
        text = ""
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
        except Exception as e:
            logging.error(f"Error reading PDF {file_path}: {e}")
        return text
    elif ext == ".docx":
        try:
            doc = docx.Document(file_path)
            return "\n".join([p.text for p in doc.paragraphs])
        except Exception as e:
            logging.error(f"Error reading DOCX {file_path}: {e}")
            return ""
    else:
        logging.warning(f"Unsupported file type: {file_path}")
        return ""

def clean_and_chunk_text(text, max_chunk_chars=500):
    """
    Clean the text and split it into smaller chunks.
    First, split by double newlines (paragraphs). Then, for any long paragraph,
    further split by period+space.
    """
    chunks = []
    paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
    for para in paragraphs:
        if len(para) > max_chunk_chars:
            sentences = para.split(". ")
            current_chunk = ""
            for sentence in sentences:
                if len(current_chunk) + len(sentence) + 1 <= max_chunk_chars:
                    current_chunk += sentence + ". "
                else:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentence + ". "
            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            chunks.append(para)
    return chunks

#########################################
# Summarization and Task Generation
#########################################

def initialize_summarizer(config):
    summarizer_model = config.get("model_config", {}).get("summarizer_model", "facebook/bart-large-cnn")
    summarizer = pipeline("summarization", model=summarizer_model)
    return summarizer

def summarize_chunk(text_chunk, summarizer, max_length=130, min_length=30):
    try:
        if len(text_chunk.split()) > 512:
            text_chunk = " ".join(text_chunk.split()[:512])
        summary = summarizer(text_chunk, max_length=max_length, min_length=min_length, do_sample=False)
        return summary[0]['summary_text']
    except Exception as e:
        logging.error(f"Summarization failed: {e}")
        return text_chunk[:100] + "..."

def filter_duplicate_tasks(tasks):
    seen = set()
    unique_tasks = []
    for task in tasks:
        desc = task["task_description"]
        if desc not in seen:
            seen.add(desc)
            unique_tasks.append(task)
    return unique_tasks

def generate_and_store_tasks_for_subject(subject, content, config, summarizer, output_folder):
    """
    Generate tasks from a subject's content by processing its text chunks.
    Tasks are accumulated and, once they reach the max_tasks threshold,
    are flushed to a new Parquet file. This process continues until all chunks are processed.
    """
    min_tasks = config["task_generation"].get("min_tasks_per_subject", 50000)
    max_tasks = config["task_generation"].get("max_tasks_per_subject", 100000)
    candidate_domains = list(config.get("subject_extraction", {}).get("keywords", {}).keys())
    
    chunks = clean_and_chunk_text(content)
    logging.info(f"Subject '{subject}' has {len(chunks)} text chunks.")
    
    file_count = 1
    current_tasks = []
    processed_chunks = 0
    
    for chunk in tqdm(chunks, desc=f"[{subject}] Processing chunks", leave=False):
        processed_chunks += 1
        summary = summarize_chunk(chunk, summarizer)
        task_description = f"Analyze the following summary: '{summary}'"
        domain = random.choice(candidate_domains)
        recommended_execution = "automated analysis" if len(chunk) > 100 else "manual review"
        current_tasks.append({
            "task_description": task_description,
            "domain": domain,
            "recommended_execution": recommended_execution,
        })
        if len(current_tasks) >= max_tasks:
            current_tasks = filter_duplicate_tasks(current_tasks)
            filename = os.path.join(output_folder, f"{subject}_{file_count}.parquet")
            pd.DataFrame(current_tasks).to_parquet(filename, index=False)
            logging.info(f"Saved {len(current_tasks)} tasks to {filename}")
            file_count += 1
            current_tasks = []
    
    if current_tasks:
        if len(current_tasks) < min_tasks:
            logging.info(f"Subject '{subject}' produced only {len(current_tasks)} tasks in final file; flushing remaining tasks.")
        current_tasks = filter_duplicate_tasks(current_tasks)
        filename = os.path.join(output_folder, f"{subject}_{file_count}.parquet")
        pd.DataFrame(current_tasks).to_parquet(filename, index=False)
        logging.info(f"Saved {len(current_tasks)} tasks to {filename}")
        file_count += 1
    
    logging.info(f"Finished generating tasks for subject '{subject}'. Total chunks processed: {processed_chunks}")

#########################################
# Parallel File Processing
#########################################

def process_file(file, subject_keywords):
    logging.info(f"Processing file: {file}")
    text = extract_text(file)
    if not text.strip():
        return None
    text_lower = text.lower()
    counts = {}
    for subject, keywords in subject_keywords.items():
        count = sum(text_lower.count(keyword.lower()) for keyword in keywords)
        counts[subject] = count
    chosen_subject = max(counts, key=counts.get)
    if counts[chosen_subject] == 0:
        chosen_subject = "General"
    return (chosen_subject, text)

#########################################
# Main Function
#########################################

def main():
    parser = argparse.ArgumentParser(description="Production Training Data Preparation Script")
    parser.add_argument("--config", type=str, default="production_config.yaml", help="Path to YAML config file")
    parser.add_argument("--no_fp16", action="store_true", help="Disable FP16 training")
    args = parser.parse_args()

    config = load_config(args.config)
    setup_logging(config.get("logging", {}))
    logging.info("Configuration loaded and logging set up.")

    print_instructions()

    input_folder = get_input_folder(config)
    if not os.path.isdir(input_folder):
        logging.error("The provided folder path is not valid.")
        sys.exit(1)

    output_folder = config.get("storage", {}).get("output_folder", os.path.join(input_folder, "parquet_output"))
    os.makedirs(output_folder, exist_ok=True)

    file_extensions = ("*.txt", "*.pdf", "*.docx")
    file_paths = []
    for ext in file_extensions:
        file_paths.extend(glob.glob(os.path.join(input_folder, ext)))
    if not file_paths:
        logging.error("No supported document files found in the folder.")
        sys.exit(1)

    subject_keywords = config.get("subject_extraction", {}).get("keywords", {})
    subjects_dict = {}
    with ThreadPoolExecutor(max_workers=config.get("parallel_processing", {}).get("num_workers", 4)) as executor:
        future_to_file = {executor.submit(process_file, file, subject_keywords): file for file in file_paths}
        for future in tqdm(as_completed(future_to_file), total=len(file_paths), desc="Ingesting files"):
            result = future.result()
            if result is None:
                continue
            subject, text = result
            subjects_dict.setdefault(subject, "")
            subjects_dict[subject] += "\n" + text

    device_config = config.get("device_config", {})
    if device_config.get("device", "auto") == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(device_config.get("device"))
    use_fp16 = (device.type == "cuda" and device_config.get("enable_fp16", True) and not args.no_fp16)
    logging.info(f"Using device: {device}. FP16 enabled: {use_fp16}")

    model_name = config.get("model_config", {}).get("model_name", "distilbert-base-uncased")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if use_fp16:
        model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16).to(device)
    else:
        model = AutoModel.from_pretrained(model_name).to(device)

    summarizer = initialize_summarizer(config)

    for subject, content in subjects_dict.items():
        logging.info(f"Generating tasks for subject: {subject}")
        generate_and_store_tasks_for_subject(subject, content, config, summarizer, output_folder)

if __name__ == "__main__":
    main()
